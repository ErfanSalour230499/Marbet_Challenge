{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66c2525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¬ Marbet Event Chatbot Ready! Type 'exit' to quit.\n",
      "\n",
      "\n",
      "ðŸ¤– Answer: I don't know what you are asking to do with Ollama or its capabilities. Please provide more context or details about your question so I can assist you better. \n",
      "\n",
      "Note that I can help you interact with the Ollama server using the Python library if needed, and also discuss how Ollama's capabilities (e.g., accuracy of responses, efficiency in retrieving information) can be applied to a particular scenario.\n",
      "ðŸ“š Source: file3.pdf\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "ðŸ¤– Answer: I'm ready to help. What is your question regarding Ollama or Retrieval-Augmented Generation (RAG)?\n",
      "ðŸ“š Source: file3.pdf\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "ðŸ¤– Answer: I don't know what the user's question is. Could you please provide more context or ask a specific question related to Ollama, RAG, or structured data?\n",
      "ðŸ“š Source: file3.pdf\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "ðŸ¤– Answer: I'm ready to help. What's your question? You can ask anything related to Ollama or Retrieval Augmentation Generation (RAG). I'll do my best to provide a clear answer based on the provided context. Go ahead and ask away!\n",
      "ðŸ“š Source: file3.pdf\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "ðŸ¤– Answer: I don't see a user's question. Please go ahead and ask your question related to Ollama or Retrieval Augmented Generation (RAG) so I can provide an answer based on the given context.\n",
      "ðŸ“š Source: file3.pdf\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "ðŸ¤– Answer: It seems like you want to use Ollama, a language model, to answer questions related to structured data. \n",
      "\n",
      "What's your question regarding Ollama or the provided context? Do you need help with interacting with the Ollama server using the Python library, structuring documents for efficient retrieval, or something else?\n",
      "ðŸ“š Source: file3.pdf\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "ðŸ¤– Answer: It seems like you're ready to interact with Ollama using the provided Python library. What would you like to do with Ollama? Would you like to:\n",
      "\n",
      "1. List the available models on the server?\n",
      "2. Ask a question and get a response based on the attendee questions?\n",
      "3. Discuss how to structure documents for efficient retrieval?\n",
      "\n",
      "Let me know, and I'll be happy to help!\n",
      "ðŸ“š Source: file3.pdf\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "ðŸ¤– Answer: It seems like there's no specific question provided. If you could provide a question related to the context about Ollama and Retrieval-Augmented Generation (RAG), I'd be happy to help answer it based on the given information.\n",
      "ðŸ“š Source: file3.pdf\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "ðŸ¤– Answer: I don't know what question you would like me to answer regarding Ollama or RAG. Please provide your question so I can assist you further!\n",
      "ðŸ“š Source: file3.pdf\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "ðŸ¤– Answer: I'm ready to help. What is your question regarding Ollama or Retrieval-Augmented Generation (RAG)?\n",
      "ðŸ“š Source: file3.pdf\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "ðŸ¤– Answer: I'm not aware of any specific question being asked. Please go ahead and ask a question, and I'll do my best to provide an answer based on the provided context about Ollama and Retrieval Augmented Generation (RAG).\n",
      "ðŸ“š Source: file3.pdf\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3_Chatbot_App.ipynb\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Step 1: Load vectorstore\n",
    "embedding = OllamaEmbeddings(\n",
    "    base_url=\"http://194.171.191.226:3061\",\n",
    "    model=\"mxbai-embed-large\"\n",
    ")\n",
    "\n",
    "vectorstore = FAISS.load_local(\"vectorstore\", embedding, allow_dangerous_deserialization=True)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Step 2: Set up LLM\n",
    "llm = ChatOllama(\n",
    "    base_url=\"http://194.171.191.226:3061\",\n",
    "    model=\"llama3.1:8b\"\n",
    ")\n",
    "\n",
    "# Step 3: Set up QA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# Step 4: Chat loop!\n",
    "print(\"ðŸ’¬ Marbet Event Chatbot Ready! Type 'exit' to quit.\\n\")\n",
    "\n",
    "while True:\n",
    "    question = input(\"You: \")\n",
    "    if question.lower() in (\"exit\", \"quit\"):\n",
    "        print(\"ðŸ‘‹ Goodbye!\")\n",
    "        break\n",
    "    result = qa_chain.invoke({\"query\": question})\n",
    "    print(\"\\nðŸ¤– Answer:\", result[\"result\"])\n",
    "    print(\"ðŸ“š Source:\", result[\"source_documents\"][0].metadata.get(\"source\", \"unknown\"))\n",
    "    print(\"\\n\" + \"-\"*60 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fa96be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1316199e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Uni_Environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
